{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "lora finetuning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch booksforcharlie/stable-diffusion-inpainting: booksforcharlie/stable-diffusion-inpainting does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 12285.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded zhengchong/CatVTON to C:\\Users\\010\\.cache\\huggingface\\hub\\models--zhengchong--CatVTON\\snapshots\\2969fcf85fe62f2036605716f0b56f0b81d01d79\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "# Add the directory containing model_pipeline to sys.path\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\",\"CatVTON\")))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\",\"CatVTON\")))\n",
    "\n",
    "from model.pipeline import CatVTONPipeline\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "#from diffusers import StableDiffusionPipeline\n",
    "# # 1. Stable Diffusion 1.5 모델 로드\n",
    "# model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\n",
    "#     model_id, torch_dtype=torch.float16\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# catvton\n",
    "base_ckpt = \"booksforcharlie/stable-diffusion-inpainting\"\n",
    "attn_ckpt = \"zhengchong/CatVTON\"\n",
    "attn_ckpt_version = \"mix\"\n",
    "\n",
    "pipe = CatVTONPipeline(\n",
    "    base_ckpt, \n",
    "    attn_ckpt,\n",
    "    attn_ckpt_version,\n",
    "    weight_dtype=torch.float32, \n",
    "    device=\"cuda\",\n",
    "    skip_safety_check=True\n",
    ")\n",
    "\n",
    "\n",
    "# 2. LoRA Config 설정 (Cross-Attention에 적용)\n",
    "lora_config = LoraConfig(\n",
    "    r=16, #8                     # LoRA Rank\n",
    "    lora_alpha=32,             # Scaling factor\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\"],  \n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# 3. LoRA 모델 적용\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커스텀 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNET_TARGET_MODULES = [\n",
    "#     \"to_q\",\n",
    "#     \"to_k\",\n",
    "#     \"to_v\",\n",
    "#     \"proj\",\n",
    "#     \"proj_in\",\n",
    "#     \"proj_out\",\n",
    "#     \"conv\",\n",
    "#     \"conv1\",\n",
    "#     \"conv2\",\n",
    "#     \"conv_shortcut\",\n",
    "#     \"to_out.0\",\n",
    "#     \"time_emb_proj\",\n",
    "#     \"ff.net.2\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 데이터 경로 설정\n",
    "data_dir = \"dataset/\"\n",
    "image_files = [f for f in os.listdir(data_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "# 이미지 변환 설정 (Stable Diffusion 입력 크기 512x512)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_dataset = []\n",
    "for img_file in image_files:\n",
    "    image_path = os.path.join(data_dir, img_file)\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "\n",
    "    # 이미지 파일명 기반으로 캡션 생성\n",
    "    caption = \"A person sitting in a wheelchair, cinematic lighting, high detail\" if \"wheelchair\" in img_file else \"A random object\"\n",
    "\n",
    "    train_dataset.append({\"image\": image, \"caption\": caption})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer = optim.AdamW(pipe.unet.parameters(), lr=1e-5)\n",
    "\n",
    "# Fine-tuning 함수\n",
    "def train_lora(pipe, dataloader, epochs=10, batch_size=32):\n",
    "    pipe.unet.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            image = batch[\"image\"].to(\"cuda\")\n",
    "            caption = batch[\"caption\"]\n",
    "\n",
    "            # ✅ UNet Forward Pass (LoRA 적용된 상태)\n",
    "            noise = torch.randn_like(image)  # 가우시안 노이즈 추가\n",
    "            noisy_image = image + 0.1 * noise\n",
    "            output = pipe.unet(noisy_image)\n",
    "\n",
    "            # ✅ 손실 계산 (MSE Loss 사용)\n",
    "            loss = ((output - image) ** 2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# LoRA Fine-tuning 실행\n",
    "train_lora(pipe, train_loader, epochs=20, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 가중치 저장\n",
    "pipe.unet.save_pretrained(\"lora_sd1.5_finetuned\")\n",
    "\n",
    "# Inference 테스트\n",
    "#prompt = \"A person sitting in a wheelchair, cinematic lighting, high detail\"\n",
    "#image = pipe(prompt, height=512, width=384).images[0]\n",
    "\n",
    "# 결과 저장\n",
    "image.save(\"wheelchair_lora_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# PEFT 라이브러리를 통해 LoRA 모듈 적용\n",
    "try:\n",
    "    from peft import get_peft_model, LoraConfig, TaskType\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install peft library: pip install peft\")\n",
    "\n",
    "# latent diffusion 관련 라이브러리 (예: diffusers의 DDPMScheduler)\n",
    "try:\n",
    "    from diffusers import DDPMScheduler\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install diffusers library: pip install diffusers\")\n",
    "\n",
    "# CatVTONPipeline 임포트 (여러분의 프로젝트 구조에 맞게 수정)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\",\"CatVTON\")))\n",
    "from model.pipeline import CatVTONPipeline\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"LoRA Fine-tuning for Latent Diffusion based CatVTON\")\n",
    "    parser.add_argument(\"--data_root_path\", type=str, required=True, help=\"Path to the training dataset.\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"output\", help=\"Directory to save checkpoints.\")\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=10, help=\"Number of training epochs.\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size for training.\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate.\")\n",
    "    parser.add_argument(\"--lora_rank\", type=int, default=4, help=\"LoRA rank parameter.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=555, help=\"Random seed for reproducibility.\")\n",
    "    # latent diffusion 관련 파라미터\n",
    "    parser.add_argument(\"--num_train_timesteps\", type=int, default=1000, help=\"Number of diffusion steps for training.\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.vae_processor = VaeImageProcessor(vae_scale_factor=8)\n",
    "        self.mask_processor = VaeImageProcessor(\n",
    "            vae_scale_factor=8, \n",
    "            do_normalize=False, \n",
    "            do_binarize=True, \n",
    "            do_convert_grayscale=True\n",
    "        )\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        return []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        person, cloth, mask = [Image.open(data[key]) for key in ['person', 'cloth', 'mask']]\n",
    "        return {\n",
    "            'index': idx,\n",
    "            'person_name': data['person_name'],\n",
    "            'person': self.vae_processor.preprocess(person, self.args.height, self.args.width)[0],\n",
    "            'cloth': self.vae_processor.preprocess(cloth, self.args.height, self.args.width)[0],\n",
    "            'mask': self.mask_processor.preprocess(mask, self.args.height, self.args.width)[0]\n",
    "        }\n",
    "\n",
    "class VITONHDTestDataset(TrainDataset):\n",
    "    def load_data(self):\n",
    "        pair_txt = os.path.join(self.args.data_root_path, 'test_pairs_unpaired.txt')\n",
    "        assert os.path.exists(pair_txt), f\"File {pair_txt} does not exist.\"\n",
    "        with open(pair_txt, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.args.data_root_path = os.path.join(self.args.data_root_path, \"test\")\n",
    "        output_dir = os.path.join(\n",
    "            self.args.output_dir, \n",
    "            \"vitonhd\", \n",
    "            'unpaired' if not self.args.eval_pair else 'paired'\n",
    "        )\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            person_img, cloth_img = line.strip().split(\" \")\n",
    "            if os.path.exists(os.path.join(output_dir, person_img)):\n",
    "                continue\n",
    "            if self.args.eval_pair:\n",
    "                cloth_img = person_img\n",
    "            data.append({\n",
    "                'person_name': person_img,\n",
    "                'person': os.path.join(self.args.data_root_path, 'image', person_img),\n",
    "                'cloth': os.path.join(self.args.data_root_path, 'cloth', cloth_img),\n",
    "                'mask': os.path.join(self.args.data_root_path, 'agnostic-mask', person_img.replace('.jpg', '_mask.png')),\n",
    "            })\n",
    "        return data\n",
    "\n",
    "class VITONHDTrainDataset(TrainDataset):\n",
    "    def load_data(self):\n",
    "        pair_txt = os.path.join(self.args.data_root_path, 'train_pairs.txt')\n",
    "        assert os.path.exists(pair_txt), f\"File {pair_txt} does not exist.\"\n",
    "        with open(pair_txt, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.args.data_root_path = os.path.join(self.args.data_root_path, \"train\")\n",
    "        output_dir = os.path.join(\n",
    "            self.args.output_dir, \n",
    "            \"vitonhd\", \n",
    "            'unpaired' if not self.args.eval_pair else 'paired'\n",
    "        )\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            person_img, cloth_img = line.strip().split(\" \")\n",
    "            if os.path.exists(os.path.join(output_dir, person_img)):\n",
    "                continue\n",
    "            if self.args.eval_pair:\n",
    "                cloth_img = person_img\n",
    "            data.append({\n",
    "                'person_name': person_img,\n",
    "                'person': os.path.join(self.args.data_root_path, 'image', person_img),\n",
    "                'cloth': os.path.join(self.args.data_root_path, 'cloth', cloth_img),\n",
    "                'mask': os.path.join(self.args.data_root_path, 'agnostic-mask', person_img.replace('.jpg', '_mask.png')),\n",
    "            })\n",
    "        return data\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1. 파이프라인과 모델 초기화 (여러분의 환경에 맞게 수정)\n",
    "    base_ckpt = \"booksforcharlie/stable-diffusion-inpainting\"\n",
    "    attn_ckpt = \"zhengchong/CatVTON\"\n",
    "    attn_ckpt_version = \"mix\"\n",
    "\n",
    "    pipeline = CatVTONPipeline(\n",
    "        base_ckpt, \n",
    "        attn_ckpt,\n",
    "        attn_ckpt_version,\n",
    "        weight_dtype=torch.bfloat16, \n",
    "        device=\"cuda\",\n",
    "        skip_safety_check=True\n",
    "    )\n",
    "\n",
    "    # fine-tuning 대상 모델 (예: UNet) 추출\n",
    "    model = pipeline.model  \n",
    "    model.to(device)\n",
    "\n",
    "    # 2. LoRA 설정 및 적용\n",
    "    lora_config = LoraConfig(\n",
    "        # task_type=TaskType.TEXT_TO_IMAGE,  # 상황에 맞게 변경\n",
    "        r=args.lora_rank,\n",
    "        lora_alpha=6,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"LoRA 적용 완료. 현재 학습 파라미터 수:\",\n",
    "          sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "    # 3. 노이즈 스케줄러 (DDPM) 초기화\n",
    "    scheduler = DDPMScheduler(num_train_timesteps=args.num_train_timesteps)\n",
    "    \n",
    "    # 4. 옵티마이저 및 손실 함수 정의\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    loss_fn = nn.MSELoss()  # diffusion 학습에서는 주로 예측한 노이즈와 실제 노이즈 간의 MSE를 사용\n",
    "\n",
    "    # 5. 데이터셋 및 DataLoader 정의\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 384)),  # 모델 입력 해상도에 맞게 조정\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    dataset = VITONHDTrainDataset(args.data_root_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(args.num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            # 원본 이미지 (예: latent로 변환하기 전 이미지)\n",
    "            images = batch.to(device)\n",
    "\n",
    "            # 5.1. VAE encoder를 통해 latent 표현을 구하는 경우\n",
    "            # latent = vae_encoder(images)  \n",
    "            # 여기서는 간단히 images를 latent로 사용한다고 가정\n",
    "            latents = images\n",
    "\n",
    "            # 5.2. 노이즈 샘플링\n",
    "            noise = torch.randn_like(latents)\n",
    "\n",
    "            # 5.3. diffusion 스케줄러로부터 random timestep 선택\n",
    "            timesteps = torch.randint(0, args.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "\n",
    "            # 5.4. 노이즈를 latent에 추가 (scheduler의 add_noise 함수 사용)\n",
    "            noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # 5.5. 모델에 noisy_latents와 timestep 정보를 입력하여 노이즈 예측\n",
    "            # 모델 구조에 따라 timestep 입력 방식은 달라질 수 있음.\n",
    "            # 예시: predicted_noise = model(noisy_latents, timesteps=timesteps)\n",
    "            predicted_noise = model(noisy_latents, timesteps=timesteps)\n",
    "\n",
    "            # 5.6. 손실 계산: 예측한 노이즈와 실제 샘플링한 노이즈의 MSE\n",
    "            loss = loss_fn(predicted_noise, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{args.num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 체크포인트 저장\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(args.output_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint 저장: {checkpoint_path}\")\n",
    "\n",
    "    print(\"학습 완료.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catvton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
