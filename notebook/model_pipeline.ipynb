{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "from typing import Union\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from accelerate import load_checkpoint_in_model\n",
    "from diffusers import AutoencoderKL, DDIMScheduler, UNet2DConditionModel\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import \\\n",
    "    StableDiffusionSafetyChecker\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "from model.attn_processor import SkipAttnProcessor\n",
    "from model.utils import get_trainable_module, init_adapter\n",
    "from utils import (compute_vae_encodings, numpy_to_pil, prepare_image,\n",
    "                   prepare_mask_image, resize_and_crop, resize_and_padding)\n",
    "\n",
    "\n",
    "class CatVTONPipeline:\n",
    "    def __init__(\n",
    "        self, \n",
    "        base_ckpt, \n",
    "        attn_ckpt, \n",
    "        attn_ckpt_version=\"mix\",\n",
    "        weight_dtype=torch.float32,\n",
    "        device='cuda',\n",
    "        compile=False,\n",
    "        skip_safety_check=False,\n",
    "        use_tf32=True,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.weight_dtype = weight_dtype\n",
    "        self.skip_safety_check = skip_safety_check\n",
    "\n",
    "        self.noise_scheduler = DDIMScheduler.from_pretrained(base_ckpt, subfolder=\"scheduler\")\n",
    "        self.vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device, dtype=weight_dtype)\n",
    "        if not skip_safety_check:\n",
    "            self.feature_extractor = CLIPImageProcessor.from_pretrained(base_ckpt, subfolder=\"feature_extractor\")\n",
    "            self.safety_checker = StableDiffusionSafetyChecker.from_pretrained(base_ckpt, subfolder=\"safety_checker\").to(device, dtype=weight_dtype)\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(base_ckpt, subfolder=\"unet\").to(device, dtype=weight_dtype)\n",
    "        init_adapter(self.unet, cross_attn_cls=SkipAttnProcessor)  # Skip Cross-Attention\n",
    "        self.attn_modules = get_trainable_module(self.unet, \"attention\")\n",
    "        self.auto_attn_ckpt_load(attn_ckpt, attn_ckpt_version)\n",
    "        # Pytorch 2.0 Compile\n",
    "        if compile:\n",
    "            self.unet = torch.compile(self.unet)\n",
    "            self.vae = torch.compile(self.vae, mode=\"reduce-overhead\")\n",
    "            \n",
    "        # Enable TF32 for faster training on Ampere GPUs (A100 and RTX 30 series).\n",
    "        if use_tf32:\n",
    "            torch.set_float32_matmul_precision(\"high\")\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    def auto_attn_ckpt_load(self, attn_ckpt, version):\n",
    "        sub_folder = {\n",
    "            \"mix\": \"mix-48k-1024\",\n",
    "            \"vitonhd\": \"vitonhd-16k-512\",\n",
    "            \"dresscode\": \"dresscode-16k-512\",\n",
    "        }[version]\n",
    "        if os.path.exists(attn_ckpt):\n",
    "            load_checkpoint_in_model(self.attn_modules, os.path.join(attn_ckpt, sub_folder, 'attention'))\n",
    "        else:\n",
    "            repo_path = snapshot_download(repo_id=attn_ckpt)\n",
    "            print(f\"Downloaded {attn_ckpt} to {repo_path}\")\n",
    "            load_checkpoint_in_model(self.attn_modules, os.path.join(repo_path, sub_folder, 'attention'))\n",
    "            \n",
    "    def run_safety_checker(self, image):\n",
    "        if self.safety_checker is None:\n",
    "            has_nsfw_concept = None\n",
    "        else:\n",
    "            safety_checker_input = self.feature_extractor(image, return_tensors=\"pt\").to(self.device)\n",
    "            image, has_nsfw_concept = self.safety_checker(\n",
    "                images=image, clip_input=safety_checker_input.pixel_values.to(self.weight_dtype)\n",
    "            )\n",
    "        return image, has_nsfw_concept\n",
    "    \n",
    "    def check_inputs(self, image, condition_image, mask, width, height):\n",
    "        if isinstance(image, torch.Tensor) and isinstance(condition_image, torch.Tensor) and isinstance(mask, torch.Tensor):\n",
    "            return image, condition_image, mask\n",
    "        assert image.size == mask.size, \"Image and mask must have the same size\"\n",
    "        image = resize_and_crop(image, (width, height))\n",
    "        mask = resize_and_crop(mask, (width, height))\n",
    "        condition_image = resize_and_padding(condition_image, (width, height))\n",
    "        return image, condition_image, mask\n",
    "    \n",
    "    def prepare_extra_step_kwargs(self, generator, eta):\n",
    "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
    "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
    "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
    "        # and should be between [0, 1]\n",
    "\n",
    "        accepts_eta = \"eta\" in set(\n",
    "            inspect.signature(self.noise_scheduler.step).parameters.keys()\n",
    "        )\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "        # check if the scheduler accepts generator\n",
    "        accepts_generator = \"generator\" in set(\n",
    "            inspect.signature(self.noise_scheduler.step).parameters.keys()\n",
    "        )\n",
    "        if accepts_generator:\n",
    "            extra_step_kwargs[\"generator\"] = generator\n",
    "        return extra_step_kwargs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self, \n",
    "        image: Union[PIL.Image.Image, torch.Tensor],\n",
    "        condition_image: Union[PIL.Image.Image, torch.Tensor],\n",
    "        mask: Union[PIL.Image.Image, torch.Tensor],\n",
    "        num_inference_steps: int = 50,\n",
    "        guidance_scale: float = 2.5,\n",
    "        height: int = 1024,\n",
    "        width: int = 768,\n",
    "        generator=None,\n",
    "        eta=1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        concat_dim = -2  # FIXME: y axis concat\n",
    "        # Prepare inputs to Tensor\n",
    "        image, condition_image, mask = self.check_inputs(image, condition_image, mask, width, height)\n",
    "        image = prepare_image(image).to(self.device, dtype=self.weight_dtype)\n",
    "        condition_image = prepare_image(condition_image).to(self.device, dtype=self.weight_dtype)\n",
    "        mask = prepare_mask_image(mask).to(self.device, dtype=self.weight_dtype)\n",
    "        # Mask image\n",
    "        masked_image = image * (mask < 0.5)\n",
    "        # VAE encoding\n",
    "        masked_latent = compute_vae_encodings(masked_image, self.vae)\n",
    "        condition_latent = compute_vae_encodings(condition_image, self.vae)\n",
    "        mask_latent = torch.nn.functional.interpolate(mask, size=masked_latent.shape[-2:], mode=\"nearest\")\n",
    "        del image, mask, condition_image\n",
    "        # Concatenate latents\n",
    "        masked_latent_concat = torch.cat([masked_latent, condition_latent], dim=concat_dim)\n",
    "        mask_latent_concat = torch.cat([mask_latent, torch.zeros_like(mask_latent)], dim=concat_dim)\n",
    "        # Prepare noise\n",
    "        latents = randn_tensor(\n",
    "            masked_latent_concat.shape,\n",
    "            generator=generator,\n",
    "            device=masked_latent_concat.device,\n",
    "            dtype=self.weight_dtype,\n",
    "        )\n",
    "        # Prepare timesteps\n",
    "        self.noise_scheduler.set_timesteps(num_inference_steps, device=self.device)\n",
    "        timesteps = self.noise_scheduler.timesteps\n",
    "        latents = latents * self.noise_scheduler.init_noise_sigma\n",
    "        # Classifier-Free Guidance\n",
    "        if do_classifier_free_guidance := (guidance_scale > 1.0):\n",
    "            masked_latent_concat = torch.cat(\n",
    "                [\n",
    "                    torch.cat([masked_latent, torch.zeros_like(condition_latent)], dim=concat_dim),\n",
    "                    masked_latent_concat,\n",
    "                ]\n",
    "            )\n",
    "            mask_latent_concat = torch.cat([mask_latent_concat] * 2)\n",
    "\n",
    "        # Denoising loop\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "        num_warmup_steps = (len(timesteps) - num_inference_steps * self.noise_scheduler.order)\n",
    "        with tqdm.tqdm(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                non_inpainting_latent_model_input = (torch.cat([latents] * 2) if do_classifier_free_guidance else latents)\n",
    "                non_inpainting_latent_model_input = self.noise_scheduler.scale_model_input(non_inpainting_latent_model_input, t)\n",
    "                # prepare the input for the inpainting model\n",
    "                inpainting_latent_model_input = torch.cat([non_inpainting_latent_model_input, mask_latent_concat, masked_latent_concat], dim=1)\n",
    "                # predict the noise residual\n",
    "                noise_pred= self.unet(\n",
    "                    inpainting_latent_model_input,\n",
    "                    t.to(self.device),\n",
    "                    encoder_hidden_states=None, # FIXME\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (\n",
    "                        noise_pred_text - noise_pred_uncond\n",
    "                    )\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.noise_scheduler.step(\n",
    "                    noise_pred, t, latents, **extra_step_kwargs\n",
    "                ).prev_sample\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or (\n",
    "                    (i + 1) > num_warmup_steps\n",
    "                    and (i + 1) % self.noise_scheduler.order == 0\n",
    "                ):\n",
    "                    progress_bar.update()\n",
    "\n",
    "        # Decode the final latents\n",
    "        latents = latents.split(latents.shape[concat_dim] // 2, dim=concat_dim)[0]\n",
    "        latents = 1 / self.vae.config.scaling_factor * latents\n",
    "        image = self.vae.decode(latents.to(self.device, dtype=self.weight_dtype)).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n",
    "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "        image = numpy_to_pil(image)\n",
    "        \n",
    "        # Safety Check\n",
    "        if not self.skip_safety_check:\n",
    "            current_script_directory = os.path.dirname(os.path.realpath(__file__))\n",
    "            nsfw_image = os.path.join(os.path.dirname(current_script_directory), 'resource', 'img', 'NSFW.jpg')\n",
    "            nsfw_image = PIL.Image.open(nsfw_image).resize(image[0].size)\n",
    "            image_np = np.array(image)\n",
    "            _, has_nsfw_concept = self.run_safety_checker(image=image_np)\n",
    "            for i, not_safe in enumerate(has_nsfw_concept):\n",
    "                if not_safe:\n",
    "                    image[i] = nsfw_image\n",
    "        return image\n",
    "\n",
    "\n",
    "class CatVTONPix2PixPipeline(CatVTONPipeline):\n",
    "    def auto_attn_ckpt_load(self, attn_ckpt, version):\n",
    "        # TODO: Temperal fix for the model version\n",
    "        if os.path.exists(attn_ckpt):\n",
    "            load_checkpoint_in_model(self.attn_modules, os.path.join(attn_ckpt, version, 'attention'))\n",
    "        else:\n",
    "            repo_path = snapshot_download(repo_id=attn_ckpt)\n",
    "            print(f\"Downloaded {attn_ckpt} to {repo_path}\")\n",
    "            load_checkpoint_in_model(self.attn_modules, os.path.join(repo_path, version, 'attention'))\n",
    "    \n",
    "    def check_inputs(self, image, condition_image, width, height):\n",
    "        if isinstance(image, torch.Tensor) and isinstance(condition_image, torch.Tensor) and isinstance(torch.Tensor):\n",
    "            return image, condition_image\n",
    "        image = resize_and_crop(image, (width, height))\n",
    "        condition_image = resize_and_padding(condition_image, (width, height))\n",
    "        return image, condition_image\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self, \n",
    "        image: Union[PIL.Image.Image, torch.Tensor],\n",
    "        condition_image: Union[PIL.Image.Image, torch.Tensor],\n",
    "        num_inference_steps: int = 50,\n",
    "        guidance_scale: float = 2.5,\n",
    "        height: int = 1024,\n",
    "        width: int = 768,\n",
    "        generator=None,\n",
    "        eta=1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        concat_dim = -1\n",
    "        # Prepare inputs to Tensor\n",
    "        image, condition_image = self.check_inputs(image, condition_image, width, height)\n",
    "        image = prepare_image(image).to(self.device, dtype=self.weight_dtype)\n",
    "        condition_image = prepare_image(condition_image).to(self.device, dtype=self.weight_dtype)\n",
    "        # VAE encoding\n",
    "        image_latent = compute_vae_encodings(image, self.vae)\n",
    "        condition_latent = compute_vae_encodings(condition_image, self.vae)\n",
    "        del image, condition_image\n",
    "        # Concatenate latents\n",
    "        condition_latent_concat = torch.cat([image_latent, condition_latent], dim=concat_dim)\n",
    "        # Prepare noise\n",
    "        latents = randn_tensor(\n",
    "            condition_latent_concat.shape,\n",
    "            generator=generator,\n",
    "            device=condition_latent_concat.device,\n",
    "            dtype=self.weight_dtype,\n",
    "        )\n",
    "        # Prepare timesteps\n",
    "        self.noise_scheduler.set_timesteps(num_inference_steps, device=self.device)\n",
    "        timesteps = self.noise_scheduler.timesteps\n",
    "        latents = latents * self.noise_scheduler.init_noise_sigma\n",
    "        # Classifier-Free Guidance\n",
    "        if do_classifier_free_guidance := (guidance_scale > 1.0):\n",
    "            condition_latent_concat = torch.cat(\n",
    "                [\n",
    "                    torch.cat([image_latent, torch.zeros_like(condition_latent)], dim=concat_dim),\n",
    "                    condition_latent_concat,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Denoising loop\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "        num_warmup_steps = (len(timesteps) - num_inference_steps * self.noise_scheduler.order)\n",
    "        with tqdm.tqdm(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = (torch.cat([latents] * 2) if do_classifier_free_guidance else latents)\n",
    "                latent_model_input = self.noise_scheduler.scale_model_input(latent_model_input, t)\n",
    "                # prepare the input for the inpainting model\n",
    "                p2p_latent_model_input = torch.cat([latent_model_input, condition_latent_concat], dim=1)\n",
    "                # predict the noise residual\n",
    "                noise_pred= self.unet(\n",
    "                    p2p_latent_model_input,\n",
    "                    t.to(self.device),\n",
    "                    encoder_hidden_states=None, \n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (\n",
    "                        noise_pred_text - noise_pred_uncond\n",
    "                    )\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.noise_scheduler.step(\n",
    "                    noise_pred, t, latents, **extra_step_kwargs\n",
    "                ).prev_sample\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or (\n",
    "                    (i + 1) > num_warmup_steps\n",
    "                    and (i + 1) % self.noise_scheduler.order == 0\n",
    "                ):\n",
    "                    progress_bar.update()\n",
    "\n",
    "        # Decode the final latents\n",
    "        latents = latents.split(latents.shape[concat_dim] // 2, dim=concat_dim)[0]\n",
    "        latents = 1 / self.vae.config.scaling_factor * latents\n",
    "        image = self.vae.decode(latents.to(self.device, dtype=self.weight_dtype)).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n",
    "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "        image = numpy_to_pil(image)\n",
    "        \n",
    "        # Safety Check\n",
    "        if not self.skip_safety_check:\n",
    "            current_script_directory = os.path.dirname(os.path.realpath(__file__))\n",
    "            nsfw_image = os.path.join(os.path.dirname(current_script_directory), 'resource', 'img', 'NSFW.jpg')\n",
    "            nsfw_image = PIL.Image.open(nsfw_image).resize(image[0].size)\n",
    "            image_np = np.array(image)\n",
    "            _, has_nsfw_concept = self.run_safety_checker(image=image_np)\n",
    "            for i, not_safe in enumerate(has_nsfw_concept):\n",
    "                if not_safe:\n",
    "                    image[i] = nsfw_image\n",
    "        return image"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
